{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set at program startup\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "print(os.cpu_count())\n",
    "\n",
    "data_dir = './data'\n",
    "output_dir = './output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#display(HTML('<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>'))\n",
    "\n",
    "res = 224\n",
    "n_of_sampled_images = 36\n",
    "n_to_select = 4\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "frames = sorted(glob.glob(os.path.join(data_dir, '*/*/frames/frames_*.jpg')))\n",
    "\n",
    "total_size = 0\n",
    "for frame in frames:\n",
    "    total_size += os.stat(frame).st_size\n",
    "    \n",
    "print(total_size)\n",
    "\n",
    "split_frames = [frame.split('/') for frame in frames]\n",
    "\n",
    "sign = [split_frame[2] for split_frame in split_frames]\n",
    "\n",
    "user = [split_frame[3].split('_')[1] for split_frame in split_frames]\n",
    "\n",
    "attempt = [split_frame[3].split('_')[2] for split_frame in split_frames]\n",
    "\n",
    "frame_num = [split_frame[5].split('_')[1].split('.')[0] for split_frame in split_frames]\n",
    "\n",
    "class Counter(dict):\n",
    "    def __missing__(self, key):\n",
    "        return 0\n",
    "\n",
    "attempts = Counter()\n",
    "\n",
    "class Accumulator(dict):\n",
    "    def __missing__(self, key):\n",
    "        return []\n",
    "    \n",
    "class Mapper(dict):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "    def __missing__(self, key):\n",
    "        self.counter += 1\n",
    "        return self.counter - 1\n",
    "\n",
    "videos = Accumulator()\n",
    "sign_ids = Mapper()\n",
    "\n",
    "for frame, s, u, a in zip(frames, sign, user, attempt):\n",
    "    attempts[s + '_' + u + '_' + a] += 1\n",
    "    videos[s + '_' + u + '_' + a] += [frame]\n",
    "    sign_ids[s] += 0\n",
    "    \n",
    "\n",
    "print(sign_ids.counter)\n",
    "print(len(attempts))\n",
    "    \n",
    "s = 0\n",
    "mn = 2 ** 30\n",
    "mx = 0\n",
    "for k, v in attempts.items():\n",
    "    s += v\n",
    "    mn = min(mn, v)\n",
    "    mx = max(mx, v)\n",
    "    \n",
    "s /= len(attempts)\n",
    "\n",
    "print(mn, s, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_video(kv):\n",
    "    k, v = kv\n",
    "    indices = set((np.linspace(1, attempts[k], n_of_sampled_images) + 0.5).astype('int64').tolist())\n",
    "    vid = []\n",
    "    vd = []\n",
    "    label = sign_ids[k.split('_')[0]]\n",
    "    print(k, label, len(v), indices)\n",
    "    for j, frame in enumerate(v):\n",
    "        if j + 1 in indices:\n",
    "            vid.append(frame)\n",
    "            im = imageio.imread(frame)[92: -92, 512: -512]\n",
    "            n_im = cv2.resize(im, (224, 224), interpolation = cv2.INTER_CUBIC)\n",
    "            vd.append(n_im)\n",
    "            #plt.imshow(n_im)\n",
    "            #plt.show()\n",
    "            #imageio.imwrite(os.path.join(output_dir, k + '_' + str(j) + '_' + str(i + 1) + '.jpg'), n_im, quality = 95)\n",
    "    return vid, vd, label\n",
    "\n",
    "vids = []\n",
    "vds = []\n",
    "labels = []\n",
    "with Pool() as pool:\n",
    "    for i, (vid, vd, label) in enumerate(pool.imap(process_video, list(videos.items()))):\n",
    "        print(i)\n",
    "        vids.append(vid)\n",
    "        vds.append(vd)\n",
    "        labels.append(label)\n",
    "    \n",
    "videos_np = np.zeros([len(videos), n_of_sampled_images, res, res, 3], np.uint8)\n",
    "labels_np = np.zeros([len(videos)], np.int64)\n",
    "for i, (video, label) in enumerate(zip(vds, labels)):\n",
    "    for j, image in enumerate(video):\n",
    "        videos_np[i, j, :, :, :] = image\n",
    "    labels_np[i] = label\n",
    "np.save('videos', videos_np)\n",
    "np.save('labels', labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "dataset_dir = '.'\n",
    "\n",
    "def load_SIGN():\n",
    "    videos = np.load(join(dataset_dir, 'videos.npy'), mmap_mode = 'r')\n",
    "    labels = np.load(join(dataset_dir, 'labels.npy'), mmap_mode = 'r')\n",
    "    assert videos.shape[0] == labels.shape[0]\n",
    "    perm = np.random.permutation(videos.shape[0])\n",
    "    videos = videos[perm]\n",
    "    labels = labels[perm]\n",
    "    N = videos.shape[0] * 9 // 10\n",
    "    return (videos[: N], labels[: N]), (videos[N:], labels[N:])\n",
    "\n",
    "tr_data, val_data = load_SIGN()\n",
    "print(tr_data[0].shape, tr_data[1].shape)\n",
    "print(val_data[0].shape, val_data[1].shape)\n",
    "print(tr_data[0].dtype, tr_data[1].dtype)\n",
    "\n",
    "plt.imshow(tr_data[0][52, 18])\n",
    "print(tr_data[1][52])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def input_fn(data, batch_size, num_epochs = 1, seed = None):\n",
    "    assert len(data[0]) == len(data[1])\n",
    "\n",
    "    def load_and_preprocess_video(video, label):\n",
    "        def load_frame(frame):\n",
    "            img_raw = tf.read_file(frame)\n",
    "            img_tensor = tf.image.decode_jpeg(img_raw)\n",
    "            return img_tensor\n",
    "        \n",
    "        video = tf.map_fn(load_frame, video, tf.uint8)\n",
    "        video = tf.image.crop_to_bounding_box(video, 92, 512, res * 4, res * 4)\n",
    "        video = tf.image.resize_bicubic(video, (res, res))\n",
    "        return (video, label)\n",
    "            \n",
    "    data = tf.data.Dataset.from_tensor_slices(data).map(load_and_preprocess_video, 8)\n",
    "    data = data.repeat(num_epochs).shuffle(100).batch(batch_size).prefetch(2)\n",
    "    return data\n",
    "\n",
    "def dataset_input_fn(data, batch_size, seed = None):\n",
    "    assert data[0].shape[0] == data[1].shape[0]\n",
    "\n",
    "    def generator():\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(data[0].shape[0])\n",
    "        for i in range(0, data[0].shape[0], batch_size):\n",
    "            ids = perm[i: i + batch_size]\n",
    "            yield data[0][ids], data[1][ids]\n",
    "    \n",
    "    def format_input(video, label):\n",
    "        video = tf.cast(video, tf.float32)\n",
    "        video = preprocess_input(video)\n",
    "        return video, label\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(generator, (tf.uint8, tf.int64)).map(format_input, tf.data.experimental.AUTOTUNE).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "tr_dataset = dataset_input_fn(tr_data, batch_size)\n",
    "val_dataset = dataset_input_fn(val_data, batch_size)\n",
    "\n",
    "for batch_id, (video_batch, label_batch) in enumerate(tr_dataset.take(3)):\n",
    "    print(batch_id, video_batch.shape, label_batch)\n",
    "    \n",
    "for batch_id, (video_batch, label_batch) in enumerate(val_dataset.take(3)):\n",
    "    print(batch_id, video_batch.shape, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_batch, label_batch in tr_dataset.take(1):\n",
    "    pass\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, k):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.k = k\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.n_to_select_from = input_shape[-2]\n",
    "        self.d1 = tf.keras.layers.Dense(100, activation = 'relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d2 = tf.keras.layers.Dense(self.n_to_select_from * self.k)\n",
    "        self.reshape = tf.keras.layers.Reshape([self.k, self.n_to_select_from], input_shape = [self.n_to_select_from * self.k])\n",
    "    \n",
    "    def call(self, inputs, temp):        \n",
    "        x = self.d1(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d2(x)\n",
    "        logits = self.reshape(x)\n",
    "        \n",
    "        def samples():\n",
    "            uniform = K.random_uniform(shape = [self.k, self.n_to_select_from], minval = np.finfo(tf.float32.as_numpy_dtype).tiny, maxval = 1.0)\n",
    "            gumbel = -K.log(-K.log(uniform))\n",
    "            noisy_logits = (logits + gumbel) / K.mean(temp)\n",
    "            samples = tf.keras.activations.softmax(noisy_logits)\n",
    "            return samples\n",
    "        \n",
    "        def discrete_logits():\n",
    "            discrete_logits = K.one_hot(K.argmax(logits, axis = -1), self.n_to_select_from)\n",
    "            return discrete_logits\n",
    "        \n",
    "        selections = K.in_train_phase(samples, discrete_logits)\n",
    "        outputs = tf.einsum('ijh,ikj->ikh', inputs, selections)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "batch = tf.keras.Input([n_of_sampled_images, res, res, 3])\n",
    "temp = tf.keras.Input([], dtype = 'float32')\n",
    "x = tf.keras.layers.Lambda(lambda x: tf.reshape(x, [-1, res, res, 3]))(batch)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape = (res, res, 3), include_top = False, weights = 'imagenet')\n",
    "base_model.trainable = False\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Lambda(lambda x: tf.reshape(x, [-1, n_of_sampled_images, 1280]))(x)\n",
    "x = MyModel(n_to_select)(inputs = x, temp = temp)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(320, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(sign_ids.counter)(x)\n",
    "x = tf.keras.layers.Softmax()(x)\n",
    "model = tf.keras.Model([batch, temp], x)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate), loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "print(K.argmax(model(input_batch)))\n",
    "print(base_model.summary())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(0.0005), loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(tr_dataset, epochs = 10, validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function   \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer, Lambda, Input, Dense, Dropout, Conv1D, MaxPool1D, Softmax, GlobalAveragePooling2D, Reshape, Flatten, Multiply\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "class Sample_Concrete(Layer):\n",
    "    \"\"\"\n",
    "    Layer for sample Concrete / Gumbel-Softmax variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, temp, k, **kwargs): \n",
    "        self.temp = temp\n",
    "        self.k = k\n",
    "        super(Sample_Concrete, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, logits):\n",
    "        # logits: [batch_size, d, k]\n",
    "        batch_size = K.shape(logits)[0]\n",
    "        d = K.shape(logits)[1]\n",
    "        logits_ = Reshape([-1, d])(logits)# [batch_size, k, d]\n",
    "        unif_shape = [batch_size, self.k, d]\n",
    "       \n",
    "        uniform = K.random_uniform(unif_shape, np.finfo(tf.float32.as_numpy_dtype).tiny, 1.0)\n",
    "        gumbel = -K.log(-K.log(uniform))\n",
    "        noisy_logits = (gumbel + logits_) / self.temp\n",
    "        samples = K.softmax(noisy_logits)\n",
    "        samples = K.max(samples, axis = 1)\n",
    "        print(samples)\n",
    "        #logits = K.reshape(logits,[-1, d])\n",
    "        #threshold = K.expand_dims(tf.math.top_k(logits, self.k, sorted = True)[0][:,-1], -1)\n",
    "        #discrete_logits = K.cast(K.greater_equal(logits,threshold),tf.float32)\n",
    "        \n",
    "        output = samples\n",
    "        #output = K.in_train_phase(samples, discrete_logits) \n",
    "        return tf.expand_dims(output, -1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], 1)\n",
    "\n",
    "for video_batch, label_batch in tr_dataset.take(1):\n",
    "    pass\n",
    "\n",
    "batch = Input([n_of_sampled_images, res, res, 3])\n",
    "x = Lambda(lambda x: K.reshape(x, [-1, res, res, 3]))(batch)\n",
    "base_model = MobileNetV2(input_shape = (res, res, 3), include_top = False, weights = 'imagenet')\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[: 100]:\n",
    "    layer.trainable = False\n",
    "x = base_model(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation = tf.nn.leaky_relu)(x)\n",
    "x = Lambda(lambda x: K.reshape(x, [-1, n_of_sampled_images, 256]))(x)\n",
    "logits = Flatten()(x)\n",
    "logits = Dense(n_of_sampled_images * n_to_select)(logits)\n",
    "logits = Reshape([-1, n_to_select])(logits)\n",
    "logits = Sample_Concrete(0.5, n_to_select)(logits)\n",
    "x = Multiply()([x, logits]) # x.shape = [-1, n_of_sampled_images, 256]\n",
    "x = Flatten()(x)\n",
    "x = Dense(sign_ids.counter, activation = tf.nn.leaky_relu)(x)\n",
    "'''\n",
    "logits = Conv1D(256, 5, activation = tf.nn.leaky_relu)(x) # logits.shape = [-1, n_of_sampled_images - 4, 256]\n",
    "logits = Conv1D(256, 5, activation = tf.nn.leaky_relu)(logits) # logits.shape = [-1, n_of_sampled_images - 8, 256]\n",
    "logits = Flatten()(logits) #logits.shape = [-1, (n_of_sampled_images - 8) * 256]\n",
    "#logits = Dropout(0.5)(logits)\n",
    "logits = Dense(n_of_sampled_images, activation = tf.nn.leaky_relu)(logits) # logits.shape = [-1, n_of_sampled_images]\n",
    "logits = Sample_Concrete(0.5, n_to_select)(logits)\n",
    "#x = Multiply()([x, logits]) # x.shape = [-1, n_of_sampled_images, 1280]\n",
    "x = Conv1D(256, 5, activation = tf.nn.leaky_relu)(x) # \n",
    "x = Conv1D(256, 5, activation = tf.nn.leaky_relu)(x)\n",
    "x = MaxPool1D()(x)\n",
    "x = Conv1D(256, 5, activation = tf.nn.leaky_relu)(x)\n",
    "x = Conv1D(256, 5, activation = tf.nn.leaky_relu)(x)\n",
    "x = MaxPool1D()(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Conv1D(256, 3, activation = tf.nn.leaky_relu)(x)\n",
    "x = Conv1D(sign_ids.counter, 1, activation = tf.nn.leaky_relu)(x)\n",
    "x = Flatten()(x)\n",
    "'''\n",
    "x = Softmax()(x)\n",
    "\n",
    "model = Model(batch, x)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = tf.losses.SparseCategoricalCrossentropy(), metrics = [tf.metrics.SparseCategoricalAccuracy()] + [tf.metrics.SparseTopKCategoricalAccuracy(i + 1, 'top_{}_accuracy'.format(i + 1)) for i in range(3)])\n",
    "print(K.argmax(model(video_batch)))\n",
    "print(base_model.summary())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(val_dataset)\n",
    "model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(tr_dataset, epochs = 50, validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(0)\n",
    "functor = K.function([model.input], [model.get_layer('sample__concrete_1').output])\n",
    "\n",
    "for video_batch, label_batch in val_dataset.take(3):\n",
    "    print([batch for batch in functor([video_batch])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, 'mini_resnet.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
